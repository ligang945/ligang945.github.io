
A.T.Field
多进程完成批量任务
A.T.Field
多进程完成批量任务
python

2015/05/14

 Share
工作中经常会遇到一类问题：

使用tshark在1000个文件中过滤中出满足某些规则的数据
解压10000个压缩文件
完成这些任务通常是把相应的命令写在脚本中，然后执行脚本，自动完成。

但是存在一个问题：通过脚本完成这些批量的任务，往往是串行的，一次执行一个任务，比较浪费时间，而服务器的性能也没有充分利用上。

让任务在后台运行，可以实现多个任务同时执行，但是这样也有一个明显的问题：并发的任务量太大，并发任务间竞争激烈，反而耗时更长。

能不能并发的完成这些任务，并能够控制并发的数量呢？

使用python的进程池，可以轻松实现。

代码如下：

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
#!/usr/bin/python
from multiprocessing import Pool
from optparse import OptionParser
import os

'''
excute cmds in cmd.txt
'''

def excute_cmd(cmd, msg):
    print(msg)
    os.system(cmd)

if __name__ == "__main__":
    parser = OptionParser()
    parser.add_option("-n", "--nums", dest="pn", default=10, type="int", help="max nums of process", metavar="INT")
    parser.add_option("-f", "--file", dest="filename", default="cmds.txt", help="read cmds from FILE", metavar="FILE")
    (options, args) = parser.parse_args()

    cmds = []
    with open(options.filename, 'r') as f:
    cmds = f.readlines()

    pool = Pool(processes=options.pn)
    nums = len(cmds)
    for i in range(nums):
    cmd = cmds[i].strip('\r\n')
    msg = '%d/%d: %s' % (i + 1, nums, cmd)
    pool.apply_async(excute_cmd, (cmd, msg))
    pool.close()
    pool.join()

    print("all process done.")
用法如下：

./muti_process.py -f cmds.txt -n 10

这样将会多进程执行cmds.txt中的命令，进程数量最多为10。

特点：

这个脚本是跨平台的，windows和linux下都可以使用。
这个脚本和具体执行的任务是无关的，只要将自己想要执行的命令写到文件中，使用这个脚本就可以批量执行了。
原文作者：ligang

原文链接：http://ligang945.github.io/2015/05/14/多进程完成批量任务/

发表日期：May 14th 2015, 8:47:00 am

更新日期：September 27th 2019, 5:02:46 pm

版权声明：All Rights Reserved. 未经许可 不得转载

Next Post
git配置文件
Previous Post
你不了解的SVN
Powered by Hexotheme Archer
PV: :)
